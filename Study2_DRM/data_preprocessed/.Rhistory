eta_squared(power_analysis, partial = TRUE, ci = 0.8) #0.03 lower limit of 60%CI.
#measures per condition
table(direct[direct$type_specific == "GS",]$val, direct[direct$type_specific == "GS",]$condition)
```
```{r, echo = TRUE, include = TRUE}
#analysis 2: with valence as a factor
#random intercepts
lmer1 <- lmer(response ~ val_effect*condition_effect*type_effect
+ (1|subject),
direct, REML = FALSE)
#random slopes
lmer2 <- lmer(response ~ val_effect*condition_effect*type_effect
+ (val_effect+type_effect|subject),
direct, REML = FALSE)
lmer3 <- lmer(response ~ val_effect*condition_effect*type_effect
+ (val_effect*type_effect|subject),
direct, REML = FALSE)
anova(lmer1, lmer2, lmer3)
#choose model2
```
There is a significant drop in deviance from Model 1 to Model 2, X²(5, 3185) = 365.21, p < .001 This indicates that including random slope effects for valence and stimulus type contributes to a better fitting model than a model with a mere random intercept effect.
```{r,  echo = FALSE, include = TRUE}
summary(lmer2, corr = FALSE)
```
Parameter Interpretation: lmer2
**Random Intercept subjects = 10.81** -> variation of intercepts across subjects, relative to the grand mean
**Random Slopes Val = 39.56** -> subject-specific gradients for US valence; there are inter-individual differences in ratings across positive and negative USs, beyond the general trend of US valence as indicted by the fixed effect.
**Random Slopes Type = 1.32** -> subject-specific generalization gradients; inter-individual differences in ratings across CSs and GSs, beyond the general trend of the stimulus type.
**Corr Random Intercept x random slopes val = -0.22** -> corr. between individual intercept and individual difference between neg and pos valence (je höher der Startwert, relativ zum grand mean, desto größer der Anstieg der ratings von Neg zu Pos Valence?)
**Corr Random Intercept x random slopes type = 0.65** -> corr. between individual intercept and individual difference between CS and GS. (je höher der Startwert, relativ zum grand mean, desto größer der Abfall der ratings von CS zu GS?)
**Intercept = 3.64** -> Grand mean (average ratings were 3.64)
**Fixed Effect Condition = -7.31** -> main effect condition: On average, ratings in the "many" condition were 7.31 points lower in the "many" condition than in the "one" condition.
**Fixed Effect Type = -3.11** -> main effect type: the estimated mean difference between stimulus types in ratings was 3.11: 3.11 points lower in for "GSs" than for "CSs". n.s.
**Fixed Effect Valence = 50.11** -> main effect valence: the estimated mean difference between positive and negative valence in ratings was 50.11: 50.11 points higher for "positive" than for "negative".
**Interaction Type x Condition = 2.01** -> interaction effect condition*type: the difference between CSs and GSs was not significantly qualified by condition, but was 2.01 points higher on average in the "one" condition than in the "many" condition.
**Interaction Val x Condition = 10.71** -> the difference between pos and neg valence was not significantly qualified by the condition, but was 10.71 points higher on average in the "many" condition than in the "one" condition.
**Interaction Val x Type = -28.85** -> the difference between pos and neg valence was qualified by the type of stimuli, being on average 28.85 points higher for a CSs than GSs.
<span style = "color:darkblue;">
**Interaction Type x Condition x Val = 22.91** -> significant three-way interaction condition x type x val: the interaction of US valence (EC effect) and stimulus type was qualified by the condition, being 22.91 points higher in the "one" condition than in the "many" condition.
*-> H1a*
</span>
```{r Parameter check, echo = TRUE, include = FALSE}
#check if interpretations make sense
lmer1_analysis <- aggregate(response ~ val*type_specific*condition, direct, mean)
aggregate(response ~ type_specific, lmer1_analysis, mean)
aggregate(response ~ val, lmer1_analysis, mean)
aggregate(response ~ condition, lmer1_analysis, mean)
aggregate(response ~ type_specific*condition, lmer1_analysis, mean) #one(9.35 - 5.23) - many(1 + 1)
aggregate(response ~ val*condition, lmer1_analysis, mean) #many(27.7 + 27.7) - one(15.08+29.67)
aggregate(response ~ val*type_specific, lmer1_analysis, mean) #CS(27.7 + 37.46) - GS(15.76+19.93)
lmer1_analysis #three-way interaction: (one: (23+41) - (7 + 17)) - (many: (31 + 33.12) - (24.46 + 22.31))
```
#### simple slopes to test the significance of the EC effect (diff between neg and pos valence) for each condition and stimulus type:
lmer(ratings ~ category size * stimulus type
+ US valence(effect coded):category size(dummy coded):stimulus type(dummy coded)
+ (stimulus type + US valence | subject))
```{r simple slopes 1, echo = TRUE, include = TRUE}
#model2
lmer2_2 <- lmer(response ~ condition_effect*type_effect + val_effect:condition:type_specific
+ (val_effect+type_effect|subject),
direct, REML = FALSE)
summary(lmer2_2, corr = FALSE)
```
Parameter Interpretation: lmer2_1
The EC effect was significant in both conditions for both CSs and GSs.
#### simple slopes to test if the EC effect (diff between neg and pos valence) differs significantly between the two category size conditions, for each stimulus type separately.
lmer(ratings ~ stimulus type * US valence + stimulus type * category size
+ US valence(effect coded) : category size(effect coded) : stimulus type(dummy coded)
+ (stimulus type + US valence | subject))
```{r simple slopes 2, echo = TRUE, include = TRUE}
#model3
lmer2_3 <- lmer(response ~ type_effect*val_effect + type_effect*condition_effect +
+ condition_effect:val_effect:type_specific
+ (val_effect+type_effect|subject),
direct, REML = FALSE)
summary(lmer2_3, corr = FALSE)
#val_effect:condition_effect:type_specificCS   -0.7493     8.0272  515.2793  -0.093  0.92567
#val_effect:condition_effect:type_specificGS   22.1622     6.5078  228.7815   3.405  0.00078 ***
#test dummy coding only
lmer2_3_dummy1 <- lmer(response ~ type_specific*val_effect*condition_effect
+ (val_effect+type_effect|subject),
direct, REML = FALSE)
summary(lmer2_3_dummy1, corr = FALSE)
#ref-cat: CS
#val_effect:condition_effect                   -0.7493     8.0272  515.2793  -0.093 0.925668
direct$type_specific2 <- factor(direct$type_specific, levels = c("GS","CS"), labels = c("GS", "CS"))
lmer2_3_dummy2 <- lmer(response ~ type_specific2*val_effect*condition_effect
+ (val_effect+type_effect|subject),
direct, REML = FALSE)
summary(lmer2_3_dummy2, corr = FALSE)
#ref.cat:GS
#val_effect:condition_effect                    22.162      6.508  228.782   3.405 0.000780 ***
anova(lmer2, lmer2_3_dummy1, lmer2_3)
```
**Interaction Condition x val x typeCS = -11.14** -> the difference of neg and pos valence is not significantly qualified by condition for CSs, but was 11.15 points higher in the "one" condition than in the "many" condition
**Interaction Condition x val x typeGS = 16.27** -> the difference of neg and pos valence is significantly qualified by condition for GSs, and was on average 16.27 points higher in the "many" condition than in the "one" condition.
<span style = "color:red;"> This confirms **H1a**. </span>
#### Plot:
```{r, echo = TRUE, include = FALSE}
#toublecheck
lmer1_analysis
#CSs: (23.1+41.8)-(31+33)many
aggregate(response ~ val*condition, direct[direct$type_specific == "CS",], mean)
#one:
aggregate(response ~ val*condition, direct[direct$type_specific == "GS",], mean)
#one:(7+17.5) - (24.46+22.31)
```
```{r plot direct ratings, echo = FALSE, include = TRUE}
#plot direct
direct_plot1 <- aggregate(response ~ condition*type_specific*val, direct, mean)
direct_plot1$se <- aggregate(response ~ condition*type_specific*val, direct, se) [[4]]
barplot1 <- ggplot(direct_plot1, aes (y = response, x = condition, fill = val)) +
facet_grid(. ~ type_specific) +
geom_col(position = position_dodge()) +
geom_errorbar(aes(ymin= response - se, ymax= response + se), width=.2,
position=position_dodge(.9)) +
ggtitle("Direct Evaluative Ratings\n") +
scale_fill_brewer(palette = "Paired") +
scale_x_discrete(name = "\n Condition") +
scale_y_continuous (name = "Ratings\n", limits = c(-50, 50)) +
theme_classic() +
labs(fill = "Valence") +
theme(plot.title = element_text (hjust = 0.5, face = "bold", size = 14),
text = element_text(size=14))
barplot1
```
```{r prep AMP, echo = FALSE, include = FALSE}
# AMP ---------------------------------------------------------------------
knitr::opts_knit$set(root.dir = DRM)
setwd("C:/Users/siskr01/GitHub/CSCond_analysis/Study1_EC/data")
indirect <- read.csv2("indirect.csv", header = TRUE)
#delete columns we don't need
indirect$X <- NULL
indirect$type <- NULL
str(indirect)
as_factor <- c("subject", "val", "condition", "measure", "type_specific", "category", "cs_selected", "target")
for (factor in as_factor){
indirect[, factor] <- as.factor(indirect[,factor])
}
indirect$type_specific <- factor(indirect$type_specific, levels = c("CS", "GS same", "GS different", "Feature", "Group"))
## randomly select 1 CS for many_one:
new_one <- indirect[indirect$condition == "one_one",]
new_many <- indirect[indirect$condition == "many_one",]
#remove the original CS evaluations
new_many <- new_many[!new_many$type_specific == "CS",]
new_indirect <- rbind(new_one, new_many)
for (subject in unique(indirect$subject)){
if (indirect$condition[indirect$subject == subject] == "many_one"){
for (cat in 1:4){
temp <- indirect[indirect$subject == subject & indirect$type_specific == "CS" & indirect$category == cat,]
select <- temp[1,]
#concat only 1 CS rating per participant
new_indirect <- rbind(new_indirect, select)
}
}
}
head(new_indirect)
temp <- new_indirect
# order data
temp <- temp[order(temp$subject, temp$val, temp$type_specific),]
# omit information we don't need
temp <- temp[!temp$type_specific =="GS different", ]
temp <- temp[!temp$type_specific == "Feature",]
## rename many_one to many and one_one to one
temp$condition <- factor(temp$condition, labels = c("one", "many"), levels = c("one_one", "many_one"))
##categorical variable: generalization as discrete
temp$type_specific <- factor(temp$type_specific, labels = c("CS", "GS"), levels = c("CS", "GS same"))
head(temp)
## effect code variables
#condition
temp$condition_effect <- 0
for (line in 1:dim(temp)[1]){
if (temp$condition[line] == "one"){
temp$condition_effect[line] <- -0.5
} else {
temp$condition_effect[line] <- 0.5
}
}
temp$condition_effect
#valence
temp$val_effect <- 0
for (line in 1:dim(temp)[1]){
if (temp$val[line] == "neg"){
temp$val_effect[line] <- -0.5
} else {
temp$val_effect[line] <- 0.5
}
}
temp$condition_effect
str(temp)
#type
temp$type_effect <- 0
for (line in 1:dim(temp)[1]){
if (temp$type_specific[line] == "CS"){
temp$type_effect[line] <- -0.5
} else {
temp$type_effect[line] <- 0.5
}
}
temp$type_specific
str(temp)
#set default to dummy coding
options(contrasts = c("contr.treatment", "contr.poly"))
### specify models
indirect <- temp
indirect$response <- as.numeric(indirect$response)
indirect <- indirect[order(indirect$subject, indirect$val, indirect$type_specific, indirect$category),]
indirectProp <- aggregate(response ~ subject + val_effect + val + type_effect + type_specific
+ condition + condition_effect, indirect, sum)
indirectProp$length <- aggregate(response ~ subject + val_effect + val + type_effect + type_specific + condition + condition_effect, indirect, length)[[8]]
indirectProp$prop <- indirectProp$response/indirectProp$length
```
### AMP
**Include Stimulus Type, Category Size and Valence as a factor, use 'pleasant' or 'unpleasan' answers as DV**
condition (-0.5 = one, 0.5 = many), type (-0.5 = CS, 0.5 = GS), val (-0.5 = neg, 0.5 = pos)
Model with random intercepts and slopes for participants:
glmer(response ~ condition (effect coded) * type (effect coded) * val (effect coded) + (type + val |subject), binomial)
```{r, echo = TRUE, include = TRUE}
#analysis 2: with valence as a factor
#random intercepts
glmer1 <- glmer(response ~ val_effect*condition_effect*type_effect
+ (1|subject),
indirect, binomial)
#random slopes
glmer2 <- glmer(response ~ val_effect*condition_effect*type_effect
+ (val_effect + type_effect|subject),
indirect, binomial)
anova(glmer1, glmer2)
#choose model2
```
There is a significant drop in deviance from Model 1 to Model 2, X²(5, 3186) = 28.326, p < .001 This indicates that including random slope effects for valence and stimulus type contributes to a better fitting model than a model with a mere random intercept effect.
```{r,  echo = FALSE, include = TRUE}
summary(glmer2, corr = FALSE)
```
Parameter Interpretation: glmer2
**Random Intercept subjects = 00 ??** -> no variation of intercepts across subjects
**Random Slopes Val = 39.56** -> subject-specific gradients for US valence; there are inter-individual differences in ratings across positive and negative USs, beyond the general trend of US valence as indicted by the fixed effect.
**Random Slopes Type = 1.32** -> subject-specific generalization gradients; inter-individual differences in ratings across CSs and GSs, beyond the general trend of the stimulus type.
**Intercept = exp(0.24) = 1.27 ** -> Grand mean (average chance to respond "pleasant" was 1.27?
**Fixed Effect Condition = exp(-0.21) = 0.81 ** -> main effect condition: The chance of a "pleasant" response for the "many" condition is 0.81 (81%) the chance of a "pleasant" response for the "one" condition -> pleasant responses occured more often in the "one" condition (58% of all responses in the "one" condition were "pleasant") than in the "many" condition (53% of times), .s.
**Fixed Effect Type = exp(-0.04) = 0.96** -> main effect type: The chance of a 'pleasant' response for GSs was 0.96 the chance of the of a "pleasant" response for CSs -> pleasant responses occurred almost completely as often for CSs (56% of all responses for CSs were "pleasant") as for GSs (55%), n.s.
**Fixed Effect Valence = exp(0.74) = 2.09** -> main effect valence: The chance of a "pleasant" response is 2.09 higher for positive valence than for negative valence (overall EC effect) -> "pleasant" responses occured more often for positive valence (64% of all responses to positively paired stimuli were "pleasant") than for negative valence (47% of times)
**Interaction Type x Condition = exp (-0.05) = 0.95** -> interaction effect condition*type: difference in chance to respond "pleasant" between CSs and GSs was almost the same (a bit bigger) in the "one" condition (58.17 - 57.83 = 0.34) than in the many condition (CS53.78 - GS52.18 = 1.6)
**Interaction Val x Condition = exp (0.29) = 1.34 ** -> the difference between the chances to respond "pleasant" for negative and positive stimuli for "many" was 1.34 times the difference between the chances to response "pleasant" for negative and positive for "one" (EC effect was greater for in the "many" condition (pos64.29% - neg42.67% = 21.62%) than in the "one" condition (pos 64.85% - neg51.15% = 13.7%))
**Interaction Val x Type = exp(-0.25) = 0.77 = ** -> the difference between the chances to response "pleasant" for negative and positive stimuli for GSs was 0.77 times the difference between the chances to response "pleasant" for negative and positive for CSs (EC effect was greater for CSs (pos66% - neg46% = 20%) than for GSs (pos 62% - neg48% = 14%), )
<span style = "color:darkblue;">
**Interaction Type x Condition x Val = exp(0.30) = 1.35 ) ** -> non-significant three-way interaction condition x type x val: the interaction of US valence (EC effect) and stimulus type was not qualified by the condition, but the interaction effect typexval for "many" was 1.35 of the interaction effect type x val for "one", indicating a smaller qualification of the EC effect by stimulus type in the "many" condition ((0.65 - 0.43) - (0.62 - 0.42) = 0.02) than in the "one" condition ((0.67-0.49) - (0.62-0.53) = .09)
*-> H1b*
</span>
```{r, echo = TRUE, include = FALSE}
#check if interpretations make sense
glmer2_analysis <- aggregate(response ~ val*type_specific*condition, indirect, mean)
aggregate(response ~ type_specific, glmer2_analysis, mean)
aggregate(response ~ val, glmer2_analysis, mean)
aggregate(response ~ condition, glmer2_analysis, mean)
aggregate(response ~ type_specific*condition, glmer2_analysis, mean) #(53.78 - 52.18)
aggregate(response ~ val*condition, glmer2_analysis, mean) #1 - (13.7/21.62)
aggregate(response ~ val*type_specific, glmer2_analysis, mean) #14/20
glmer2_analysis #three-way interaction: (one: (0.67-0.49) - (0.62-0.53)) - (many: (0.65 - 0.43) - (0.62 - 0.42))
```
#### simple slopes to test the significance of the EC effect (diff between neg and pos valence) for each condition and stimulus type:
glmer(ratings ~ category size * stimulus type
+ US valence(effect coded):category size(dummy coded):stimulus type(dummy coded)
+ (stimulus type * US valence | subject), binomial)
```{r, echo = TRUE, include = TRUE}
#model2
glmer2_2 <- glmer(response ~ condition_effect*type_effect + val_effect:condition:type_specific
+ (val_effect+type_effect|subject),
indirect, binomial)
summary(glmer2_2, corr = FALSE)
```
Parameter Interpretation: glmer2_1
The EC effect was significant in both conditions for both CSs and GSs, as indicated by significant positive parameter estimates for the simple slopes (chance to response "pleasant' was higher for positive valence than negative valence)
#### simple slopes to test if the EC effect (diff between neg and pos valence) differs significantly between the two category size conditions, for each stimulus type separately.
glmer(ratings ~ stimulus type * US valence + stimulus type * category size
+ US valence(effect coded) : category size(effect coded) : stimulus type(dummy coded)
+ (stimulus type * US valence | subject))
```{r, echo = TRUE, include = TRUE}
#model3
glmer2_3 <- glmer(response ~ type_effect*val_effect + type_effect*condition_effect +
+ condition_effect:val_effect:type_specific
+ (val_effect+type_effect|subject),
indirect, binomial)
summary(glmer2_3, corr = FALSE)
```
**Interaction Condition x val x typeCS = exp(0.14) = 1.15** -> the difference in chances to respond 'pleasant' to neg and pos valence is not significantly qualified by condition for CSs, but the chance in the "many" condition was 1.15 the chance in the "one" condition, indicating a greater EC effect in the "many" condition (64.65%pos-42.93%neg = 21.72) than in the "one" condition (67.33%pos-49%neg = 18.33), n.s.
**Interaction Condition x val x typeGS = exp(0.44) = 1.55** -> the difference in chances to respond 'pleasant' to neg and pos valence is significantly qualified by condition for GSs, and the chance in the "many" condition was 1.55 the chance in the "one" condition, indicating a greater EC effect in the "many" condition (61.95%pos-42.42%neg = 19.53) than in the "one" condition (62.38%pos-53.3%neg = 9.88), n.s.
<span style = "color:red;"> This confirms **H1b**. </span>
#### Plot:
```{r, echo = TRUE, include = FALSE}
#toublecheck
glmer2_analysis
#CSs: 21.75/(18.33/100)?
#GSs: 9.88/19.53
```
```{r, echo = FALSE, include = TRUE}
#plot indirect
indirect_plot1 <- aggregate(response ~ condition*type_specific*val, indirect, mean)
indirect_plot1$se <- aggregate(response ~ condition*type_specific*val, indirect, se) [[4]]
barplot2 <- ggplot(indirect_plot1, aes (y = response, x = condition, fill = val)) +
facet_grid(. ~ type_specific) +
geom_col(position = position_dodge()) +
geom_errorbar(aes(ymin= response - se, ymax= response + se), width=.2,
position=position_dodge(.9)) +
geom_hline(yintercept = 0.5) +
ggtitle("AMP\n") +
scale_fill_brewer(palette = "Paired") +
scale_x_discrete(name = "\n Condition") +
scale_y_continuous (name = "'pleasant' response rates\n", limits = c(0,1)) +
theme_classic() +
labs(fill = "Valence") +
theme(plot.title = element_text (hjust = 0.5, face = "bold", size = 14),
text = element_text(size=14))
barplot2
```
```{r, echo = FALSE, include = FALSE}
# recognition memory ------------------------------------------------------
setwd(DRM)
memory <- read.csv2("memory1.csv", header = TRUE)
str(memory)
#delete columns we don't need
memory$X <- NULL
memory$memoryCorrect <- NULL
memory$trial_index <- NULL
memory$task <- NULL
memory$rt <- NULL
#rename condition
names(memory)[names(memory) == "condition1"] <- "condition"
#delete trials with timeout
memory <- memory[!memory$timeout == "true",]
str(memory)
#as factor
as_factor <- c("subject", "condition", "type", "category", "cs_selected")
for (factor in as_factor){
memory[, factor] <- as.factor(memory[,factor])
}
temp <- memory
# omit information we don't need
temp <- temp[!temp$type =="CSpred", ]
temp <- temp[!temp$type =="CSnonpred", ]
temp <- temp[!temp$type =="GSnew", ]
temp <- temp[!temp$type == "distractor",]
temp <- temp[!temp$condition == "many_fill",]
# order data
temp <- temp[order(temp$subject, temp$type),]
## rename many_one to many and one_one to one
temp$condition <- factor(temp$condition, labels = c("one", "many"), levels = c("one_one", "many_one"))
##categorical variable: generalization as discrete
temp$type <- factor(temp$type, labels = c("CS", "GS"), levels = c("CS", "GSold"))
head(temp)
## effect code variables
#condition, one = 0.5
temp$condition_effect <- 0
for (line in 1:dim(temp)[1]){
if (temp$condition[line] == "one"){
temp$condition_effect[line] <- -0.5
} else {
temp$condition_effect[line] <- 0.5
}
}
temp$condition_effect
#type, GS = 0.5
temp$type_effect <- 0
for (line in 1:dim(temp)[1]){
if (temp$type[line] == "CS"){
temp$type_effect[line] <- -0.5
} else  {
temp$type_effect[line] <- 0.5
}
}
temp$type_effect
str(temp)
memory <- temp
memory$memoryResp <- as.numeric(memory$memoryResp)
memory <- memory[order(memory$subject, memory$type, memory$category),]
str(memory)
#for plotting
memoryProp <- aggregate(memoryResp ~ subject + type + condition, memory, sum)
memoryProp$length <- aggregate(memoryResp ~ subject + type + condition, memory, length)[[4]]
memoryProp$prop <- memoryProp$memoryResp/memoryProp$length
hist(memoryProp$prop)
head(memoryProp)
##### analysis 1: glmers
#set default to dummy coding
options(contrasts = c("contr.treatment", "contr.poly"))
```
**Analysis of DRM Results**
**H2**: Higher false memory rates when the category size is large, in comparison to the small category size. This translates into a significant category size x stimulus type interaction. Specifically, we hypothesize the 'old' response rate to be higher in the "many" condition than in the "one" condition, for GSs (s. positive parameter estimate), but not for CSs (n.s. parameter estimate) We expect this for the recognition memory measure.
for memory responses (1 = "old", 0 = "new")
condition (-0.5 = one, 0.5 = many), type (-0.5 = CS, 0.5 = GS)
Model with random intercepts for participants and stimulus:
glmer(memoryResp ~ condition* type + ( type |subject), binomial)
*signifikant interaction of fixed effects: H2*
```{r, echo = TRUE, include = TRUE}
#random intercept for participants
memory_glmer1 <- glmer(memoryResp ~ condition_effect * type_effect
+ (1|subject),
memory, binomial)
#random intercept and slope for stimuli for participants
memory_glmer2 <- glmer(memoryResp ~ condition_effect*type_effect
+ (type_effect|subject),
memory, binomial)
anova(memory_glmer1, memory_glmer2, test = "LRT")
```
There is no significant drop in deviance from Model 1 to Model 2, X²(2, 850) = 2.42, p = .297 This indicates that including a random slope for stimulus type does not lead to a better fitting model.
```{r, echo = TRUE, include = TRUE}
#model choice: glmer1
summary(memory_glmer1, corr = FALSE)
```
Parameter Interpretation: memory_glmer1
**Random Intercept subjects = 0.26** -> substantial variation of intercepts across subjects
**Intercept = exp(0.704) = 2.02 ** -> Grand mean (average chance to respond "pleasant" was 1.27?
**Fixed Effect Condition = exp(0.077) = 1.08 ** -> main effect condition: The chance of a "old" response in the "many" condition is 1.08 the chance of a "old" response for the "one" condition -> old responses occurred more often in the "many" condition (65% of all responses in the "many" condition were "pleasant") than in the "one" condition (53% of times), n.s.
**Fixed Effect Type = exp(-3.0) = 0.49** -> main effect type: The chance of a 'old' response for GSs was 0.49 the chance of the of a "old" response for CSs -> old responses occurred almost twice as often for CSs (88% of all responses for CSs were "pleasant") as for GSs (56%), s.
<span style = "color:darkblue;">
**Interaction Type x Condition = exp (2.64) = 14.01** -> interaction effect condition*type: difference in chance to respond "old" to CSs and GSs was qualified by condition, being smaller for in the "many" condition ((0.826-0.476) = 0.35) than in the "one" condition (0.942-0.186 = 0.75)
*-> H2*
</span>
```{r, echo = FALSE, include = FALSE}
#check slopes
memory_check <- aggregate(memoryResp ~ type*condition, memory, mean)
aggregate(memoryResp ~ type, memory_check, mean)
aggregate(memoryResp ~ condition, memory_check, mean)
memory_check
```
#### simple slopes to test the difference between conditions for each stimulus type separately
simple slopes:
Model with random intercepts for participants and random intercepts for targets:
Model2_1 <- glmer(ratings ~ stimulus type
+ category size : stimulus type(dummy coded)
+ (stimulus type | subject))
```{r, echo = TRUE, include = TRUE}
#analyze simple slopes (even though three-way-interaction is not significant)
memory_glmer1_1 <- glmer(memoryResp ~ type_effect
+ condition_effect:type
+ (1|subject),
memory, binomial)
summary(memory_glmer1_1, corr = FALSE)
```
**main effect condition for CS = exp(-1.24) = 0.29** -> The chance of an "old" response in the "many" condition is 0.29 the chance of a "old" response for the "one" condition for CSs -> old responses occurred more often in the "one" condition (94% of all responses in the "one" condition for CSs were "pleasant") than in the "many" condition (83% of times), s.
**main effect condition for GS = exp(1.40) = 4.05** -> The chance of an "old" response in the "many" condition is 4.05 the chance of a "old" response for the "one" condition for GSs -> old responses occurred more often in the "many" condition (47% of all responses in the "many" condition for GSs were "pleasant") than in the "one" condition (19% of times), s.
<span style = "color:red;"> This confirms **H2**. </span>
```{r, echo = FALSE, include = TRUE}
#plot model1
memory_plot2 <- aggregate(memoryResp ~ condition + type, memory, mean)
memory_plot2$se <- aggregate(memoryResp ~ condition + type, memory, se)[[3]]
barplot1 <- ggplot(memory_plot2, aes (y = memoryResp, x = type, fill = condition)) +
geom_col(position = position_dodge()) +
geom_errorbar(aes(ymin= memoryResp - se, ymax= memoryResp + se), width=.2,
position=position_dodge(.9)) +
ggtitle("DRM\n") +
scale_fill_brewer(palette = "Paired") +
scale_x_discrete(name = "\n Type") +
scale_y_continuous (name = '"old" response rates\n') +
theme_classic() +
labs(fill = "Condition") +
theme(plot.title = element_text (hjust = 0.5, face = "bold", size = 14),
text = element_text(size=14))
barplot1
```
knitr::opts_chunk$set(echo = TRUE)
#check assumptions
plot(lmer2_3)
plot(lmer2_3)
qqnorm(resid(lmer2_3))
qqnorm(resid(lmer2_3))
library(lattice)
plot(lmer2_3)
install.packages("coefplot2",repos="http://r-forge.r-project.org")
plot(lmer2)
plot(lmer2_3)
plot(glmer2)
qqnorm(resid(glmer2))
qqmath(glmer2)
qqmath(lmer2)
qqnorm(resid(lmer2))
qqmath(lmer2)
qqnorm(resid(lmer2))
qqmath(lmer2)
qqnorm(resid(lmer2))
library(effects)
install.packages("effects")
library(effects)
library(sjPlot)
install.packages("sjPlot")
library(sjPlot)
plot_model(lmer2)
plot_model(lmer2, type = "diag")
install.packages("glmmTMB")
library(glmmTMB)
plot_model(lmer2, type = "diag")
plot_model(lmer2, type = "diag")[[1]]
plot_model(lmer2, type = "diag")[[2]]
plot_model(lmer2, type = "diag")[[3]]
plot_model(lmer2, type = "diag")[[4]]
plot_model(lmer2, type = "diag")
plot_model(lmer2, type = "diag")[[1]]
plot_model(lmer2, type = "diag")[[2]]
plot_model(lmer2, type = "diag")[[3]]
plot_model(lmer2, type = "diag")[[4]]
plot_model(glmer2)
glmer2
#check assumptions
plot_model(glmer2, type = "diag")[[1]]
#check assumptions
plot_model(glmer2, type = "diag")[[1]]
plot_model(glmer2, type = "diag")[[2]]
#check assumptions
plot_model(glmer2, type = "diag")
plot_model(memory_glmer1)
#check assumptions
plot_model(memory_glmer1, type = "diag")
